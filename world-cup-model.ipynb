{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f7611a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6507451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85a233dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = df[df['tournament'].str.contains('World Cup', case=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d642da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wc[['date','home_team','away_team','home_score','away_score','neutral']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51861ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9443 entries, 1486 to 48331\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   date        9443 non-null   datetime64[ns]\n",
      " 1   home_team   9443 non-null   object        \n",
      " 2   away_team   9443 non-null   object        \n",
      " 3   home_score  9443 non-null   int64         \n",
      " 4   away_score  9443 non-null   int64         \n",
      " 5   neutral     9443 non-null   bool          \n",
      "dtypes: bool(1), datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 451.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           date  home_team      away_team  home_score  away_score  neutral\n",
       " 1486 1930-07-13    Belgium  United States           0           3     True\n",
       " 1487 1930-07-13     France         Mexico           4           1     True\n",
       " 1488 1930-07-14     Brazil     Yugoslavia           1           2     True\n",
       " 1489 1930-07-14       Peru        Romania           1           3     True\n",
       " 1490 1930-07-15  Argentina         France           1           0     True,\n",
       " None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.head(), wc.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da6781a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (9443, 6)\n",
      "\n",
      "Result distribution:\n",
      "H    4770\n",
      "A    2670\n",
      "D    2003\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique teams: 464\n",
      "\n",
      "Date range: 1930-07-13 00:00:00 to 2025-06-10 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the data distribution\n",
    "print(\"Dataset shape:\", wc.shape)\n",
    "print(\"\\nResult distribution:\")\n",
    "print(wc.apply(lambda r: 'H' if r.home_score > r.away_score else ('A' if r.home_score < r.away_score else 'D'), axis=1).value_counts())\n",
    "print(\"\\nUnique teams:\", wc['home_team'].nunique() + wc['away_team'].nunique())\n",
    "print(\"\\nDate range:\", wc['date'].min(), \"to\", wc['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa0d5a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc['home_win_rate'] = np.nan\n",
    "wc['away_win_rate'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e28bdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing historical features (this may take a moment)...\n",
      "Done!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Fix data leakage: compute win rates only from matches BEFORE current match\n",
    "wc = wc.sort_values('date').reset_index(drop=True)\n",
    "wc['home_win_rate'] = 0.0\n",
    "wc['away_win_rate'] = 0.0\n",
    "wc['home_recent_form'] = 0.0  # Win rate in last 5 matches\n",
    "wc['away_recent_form'] = 0.0\n",
    "wc['head_to_head_home'] = 0.0  # Historical H2H advantage for home team\n",
    "\n",
    "def compute_historical_features(df):\n",
    "    for i in range(len(df)):\n",
    "        current_date = df.iloc[i]['date']\n",
    "        home_team = df.iloc[i]['home_team']\n",
    "        away_team = df.iloc[i]['away_team']\n",
    "        \n",
    "        # Get all matches before current date\n",
    "        past_matches = df[df['date'] < current_date]\n",
    "        \n",
    "        # Home team historical performance\n",
    "        home_past = past_matches[\n",
    "            (past_matches['home_team'] == home_team) | \n",
    "            (past_matches['away_team'] == home_team)\n",
    "        ]\n",
    "        \n",
    "        if len(home_past) > 0:\n",
    "            home_wins = 0\n",
    "            for _, match in home_past.iterrows():\n",
    "                if match['home_team'] == home_team and match['home_score'] > match['away_score']:\n",
    "                    home_wins += 1\n",
    "                elif match['away_team'] == home_team and match['away_score'] > match['home_score']:\n",
    "                    home_wins += 1\n",
    "            df.at[i, 'home_win_rate'] = home_wins / len(home_past)\n",
    "            \n",
    "            # Recent form (last 5 matches)\n",
    "            recent_home = home_past.tail(5)\n",
    "            if len(recent_home) > 0:\n",
    "                recent_wins = 0\n",
    "                for _, match in recent_home.iterrows():\n",
    "                    if match['home_team'] == home_team and match['home_score'] > match['away_score']:\n",
    "                        recent_wins += 1\n",
    "                    elif match['away_team'] == home_team and match['away_score'] > match['home_score']:\n",
    "                        recent_wins += 1\n",
    "                df.at[i, 'home_recent_form'] = recent_wins / len(recent_home)\n",
    "        \n",
    "        # Away team historical performance\n",
    "        away_past = past_matches[\n",
    "            (past_matches['home_team'] == away_team) | \n",
    "            (past_matches['away_team'] == away_team)\n",
    "        ]\n",
    "        \n",
    "        if len(away_past) > 0:\n",
    "            away_wins = 0\n",
    "            for _, match in away_past.iterrows():\n",
    "                if match['home_team'] == away_team and match['home_score'] > match['away_score']:\n",
    "                    away_wins += 1\n",
    "                elif match['away_team'] == away_team and match['away_score'] > match['home_score']:\n",
    "                    away_wins += 1\n",
    "            df.at[i, 'away_win_rate'] = away_wins / len(away_past)\n",
    "            \n",
    "            # Recent form (last 5 matches)\n",
    "            recent_away = away_past.tail(5)\n",
    "            if len(recent_away) > 0:\n",
    "                recent_wins = 0\n",
    "                for _, match in recent_away.iterrows():\n",
    "                    if match['home_team'] == away_team and match['home_score'] > match['away_score']:\n",
    "                        recent_wins += 1\n",
    "                    elif match['away_team'] == away_team and match['away_score'] > match['home_score']:\n",
    "                        recent_wins += 1\n",
    "                df.at[i, 'away_recent_form'] = recent_wins / len(recent_away)\n",
    "        \n",
    "        # Head-to-head record\n",
    "        h2h = past_matches[\n",
    "            ((past_matches['home_team'] == home_team) & (past_matches['away_team'] == away_team)) |\n",
    "            ((past_matches['home_team'] == away_team) & (past_matches['away_team'] == home_team))\n",
    "        ]\n",
    "        \n",
    "        if len(h2h) > 0:\n",
    "            home_h2h_wins = 0\n",
    "            for _, match in h2h.iterrows():\n",
    "                if match['home_team'] == home_team and match['home_score'] > match['away_score']:\n",
    "                    home_h2h_wins += 1\n",
    "                elif match['away_team'] == home_team and match['away_score'] > match['home_score']:\n",
    "                    home_h2h_wins += 1\n",
    "            df.at[i, 'head_to_head_home'] = home_h2h_wins / len(h2h)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Computing historical features (this may take a moment)...\")\n",
    "wc = compute_historical_features(wc)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Convert neutral to int\n",
    "wc['neutral'] = wc['neutral'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "772862d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced features computed!\n"
     ]
    }
   ],
   "source": [
    "# Add more sophisticated features\n",
    "wc['goal_difference'] = wc['home_score'] - wc['away_score']\n",
    "\n",
    "# Calculate historical goal averages (excluding current match)\n",
    "wc['home_avg_goals_scored'] = 0.0\n",
    "wc['home_avg_goals_conceded'] = 0.0\n",
    "wc['away_avg_goals_scored'] = 0.0\n",
    "wc['away_avg_goals_conceded'] = 0.0\n",
    "\n",
    "for i in range(len(wc)):\n",
    "    current_date = wc.iloc[i]['date']\n",
    "    home_team = wc.iloc[i]['home_team']\n",
    "    away_team = wc.iloc[i]['away_team']\n",
    "    \n",
    "    # Get past matches for goal averages\n",
    "    past_matches = wc[wc['date'] < current_date]\n",
    "    \n",
    "    # Home team goal statistics\n",
    "    home_matches = past_matches[\n",
    "        (past_matches['home_team'] == home_team) | \n",
    "        (past_matches['away_team'] == home_team)\n",
    "    ]\n",
    "    \n",
    "    if len(home_matches) > 0:\n",
    "        home_goals_scored = 0\n",
    "        home_goals_conceded = 0\n",
    "        for _, match in home_matches.iterrows():\n",
    "            if match['home_team'] == home_team:\n",
    "                home_goals_scored += match['home_score']\n",
    "                home_goals_conceded += match['away_score']\n",
    "            else:\n",
    "                home_goals_scored += match['away_score']\n",
    "                home_goals_conceded += match['home_score']\n",
    "        \n",
    "        wc.at[i, 'home_avg_goals_scored'] = home_goals_scored / len(home_matches)\n",
    "        wc.at[i, 'home_avg_goals_conceded'] = home_goals_conceded / len(home_matches)\n",
    "    \n",
    "    # Away team goal statistics\n",
    "    away_matches = past_matches[\n",
    "        (past_matches['home_team'] == away_team) | \n",
    "        (past_matches['away_team'] == away_team)\n",
    "    ]\n",
    "    \n",
    "    if len(away_matches) > 0:\n",
    "        away_goals_scored = 0\n",
    "        away_goals_conceded = 0\n",
    "        for _, match in away_matches.iterrows():\n",
    "            if match['home_team'] == away_team:\n",
    "                away_goals_scored += match['home_score']\n",
    "                away_goals_conceded += match['away_score']\n",
    "            else:\n",
    "                away_goals_scored += match['away_score']\n",
    "                away_goals_conceded += match['home_score']\n",
    "        \n",
    "        wc.at[i, 'away_avg_goals_scored'] = away_goals_scored / len(away_matches)\n",
    "        wc.at[i, 'away_avg_goals_conceded'] = away_goals_conceded / len(away_matches)\n",
    "\n",
    "# Add year feature (tournament era effect)\n",
    "wc['year'] = wc['date'].dt.year\n",
    "wc['modern_era'] = (wc['year'] >= 1990).astype(int)  # Modern football era\n",
    "\n",
    "print(\"Enhanced features computed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "290dd60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(r):\n",
    "    if r.home_score > r.away_score: return 'H'\n",
    "    if r.home_score < r.away_score: return 'A'\n",
    "    return 'D'\n",
    "wc['result'] = wc.apply(label, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eefa4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "wc['y'] = le.fit_transform(wc['result'])  # H→?, A→?, D→?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b3d1da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after filtering: 9443 matches\n",
      "Feature correlation with target:\n",
      "home_win_rate: 0.316\n",
      "away_win_rate: -0.298\n",
      "neutral: -0.089\n",
      "home_recent_form: 0.222\n",
      "away_recent_form: -0.239\n",
      "head_to_head_home: 0.299\n",
      "home_avg_goals_scored: 0.263\n",
      "home_avg_goals_conceded: -0.286\n",
      "away_avg_goals_scored: -0.262\n",
      "away_avg_goals_conceded: 0.245\n",
      "modern_era: -0.033\n"
     ]
    }
   ],
   "source": [
    "# Use enhanced feature set\n",
    "features = [\n",
    "    'home_win_rate', 'away_win_rate', 'neutral',\n",
    "    'home_recent_form', 'away_recent_form', 'head_to_head_home',\n",
    "    'home_avg_goals_scored', 'home_avg_goals_conceded',\n",
    "    'away_avg_goals_scored', 'away_avg_goals_conceded',\n",
    "    'modern_era'\n",
    "]\n",
    "\n",
    "# Remove rows with insufficient historical data\n",
    "wc_filtered = wc.dropna(subset=features)\n",
    "print(f\"Dataset size after filtering: {len(wc_filtered)} matches\")\n",
    "\n",
    "X = wc_filtered[features].fillna(0)\n",
    "y = wc_filtered['y']\n",
    "\n",
    "print(\"Feature correlation with target:\")\n",
    "for feature in features:\n",
    "    corr = X[feature].corr(y)\n",
    "    print(f\"{feature}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7cf0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "531293f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Improved model with better hyperparameters\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features for better performance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create ensemble model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "# Ensemble model\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_model),\n",
    "        ('gb', gb_model),\n",
    "        ('lr', lr_model)\n",
    "    ],\n",
    "    voting='soft'  # Use probabilities\n",
    ")\n",
    "\n",
    "ensemble_model.fit(X_scaled, y)\n",
    "print(\"Ensemble model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df84713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble model predictions completed!\n"
     ]
    }
   ],
   "source": [
    "# Update train-test split with scaled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "y_pred_proba = ensemble_model.predict_proba(X_test)\n",
    "\n",
    "print(\"Ensemble model predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18f1a100",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, f1_score\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Comprehensive evaluation\u001b[39;00m\n\u001b[32m      5\u001b[39m accuracy = accuracy_score(y_test, y_pred)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Comprehensive evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"F1-Score (weighted): {f1:.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nActual class distribution in test set:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index())\n",
    "print(\"\\nPredicted class distribution:\")\n",
    "print(pd.Series(y_pred).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "459017da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.54      0.53      0.54       533\n",
      "           D       0.25      0.19      0.21       414\n",
      "           H       0.67      0.74      0.70       942\n",
      "\n",
      "    accuracy                           0.56      1889\n",
      "   macro avg       0.49      0.49      0.48      1889\n",
      "weighted avg       0.54      0.56      0.55      1889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = rf_model.feature_importances_\n",
    "feature_names = features\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance_df, x='importance', y='feature')\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 most important features:\")\n",
    "print(importance_df.head())\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for deployment\n",
    "import joblib\n",
    "\n",
    "# Save model components\n",
    "joblib.dump(ensemble_model, 'world_cup_model.pkl')\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "# Create prediction function for deployment\n",
    "def predict_match(home_team_stats, away_team_stats, is_neutral=0):\n",
    "    \"\"\"\n",
    "    Predict match outcome given team statistics\n",
    "    \n",
    "    Parameters:\n",
    "    home_team_stats: dict with keys:\n",
    "        - win_rate, recent_form, avg_goals_scored, avg_goals_conceded\n",
    "    away_team_stats: dict with keys:\n",
    "        - win_rate, recent_form, avg_goals_scored, avg_goals_conceded\n",
    "    is_neutral: 1 if neutral venue, 0 otherwise\n",
    "    \n",
    "    Returns:\n",
    "    - prediction: 'H', 'A', or 'D'\n",
    "    - probabilities: dict with probabilities for each outcome\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create feature vector\n",
    "    features_vector = [\n",
    "        home_team_stats['win_rate'],\n",
    "        away_team_stats['win_rate'],\n",
    "        is_neutral,\n",
    "        home_team_stats['recent_form'],\n",
    "        away_team_stats['recent_form'],\n",
    "        0.5,  # head_to_head_home (default)\n",
    "        home_team_stats['avg_goals_scored'],\n",
    "        home_team_stats['avg_goals_conceded'],\n",
    "        away_team_stats['avg_goals_scored'],\n",
    "        away_team_stats['avg_goals_conceded'],\n",
    "        1  # modern_era (assume modern)\n",
    "    ]\n",
    "    \n",
    "    # Scale features\n",
    "    features_scaled = scaler.transform([features_vector])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = ensemble_model.predict(features_scaled)[0]\n",
    "    probabilities = ensemble_model.predict_proba(features_scaled)[0]\n",
    "    \n",
    "    # Convert to readable format\n",
    "    result = le.inverse_transform([prediction])[0]\n",
    "    prob_dict = {\n",
    "        le.classes_[i]: prob for i, prob in enumerate(probabilities)\n",
    "    }\n",
    "    \n",
    "    return result, prob_dict\n",
    "\n",
    "# Example usage\n",
    "example_home = {\n",
    "    'win_rate': 0.65,\n",
    "    'recent_form': 0.8,\n",
    "    'avg_goals_scored': 2.1,\n",
    "    'avg_goals_conceded': 0.9\n",
    "}\n",
    "\n",
    "example_away = {\n",
    "    'win_rate': 0.55,\n",
    "    'recent_form': 0.6,\n",
    "    'avg_goals_scored': 1.8,\n",
    "    'avg_goals_conceded': 1.2\n",
    "}\n",
    "\n",
    "prediction, probabilities = predict_match(example_home, example_away, is_neutral=1)\n",
    "print(f\"\\nExample prediction: {prediction}\")\n",
    "print(\"Probabilities:\")\n",
    "for outcome, prob in probabilities.items():\n",
    "    print(f\"  {outcome}: {prob:.3f}\")\n",
    "\n",
    "print(f\"\\nModel accuracy improved to: {accuracy:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
